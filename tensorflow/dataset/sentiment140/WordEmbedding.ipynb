{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37322ee1-af1c-45f3-98ee-d19b7041a71d",
   "metadata": {},
   "source": [
    "# Tensorflow Dataset (sentiment140)\n",
    "\n",
    "[Catalog for sentiment140](https://www.tensorflow.org/datasets/catalog/sentiment140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5599d70a-7e9e-4601-b119-1e6f72079d2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://www.tensorflow.org/datasets/overview?hl=zh-tw\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "keras = tf.keras\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c76bf4d-888f-45e3-9d1f-9e9866a97133",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c194a29-1e3f-4e7e-b477-103e52eea265",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# https://www.tensorflow.org/datasets/splits?hl=zh-tw\n",
    "# https://www.tensorflow.org/datasets/overview?hl=zh-tw#as_tuple_as_supervisedtrue\n",
    "\n",
    "# train_ds, valid_ds = tfds.load('sentiment140', split = ['train', 'test'])\n",
    "train_ds = tfds.load('sentiment140', split = 'train[0:5%]', as_supervised = True)\n",
    "valid_ds = tfds.load('sentiment140', split = 'train[5%:6%]', as_supervised = True)\n",
    "test_ds = tfds.load('sentiment140', split = 'train[-20:]', as_supervised = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a9f16d3-1744-4227-9c23-041bf5e5b210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((), ()), types: (tf.string, tf.int32)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fc4cd1d-ccf3-48fc-a186-cf739ba8f26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b\"i'm 10x cooler than all of you! \", shape=(), dtype=string)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "i'm 10x cooler than all of you! \n"
     ]
    }
   ],
   "source": [
    "# https://www.tensorflow.org/api_docs/python/tf/data/Dataset#take\n",
    "for text, label in train_ds.take(1):\n",
    "    print(text)\n",
    "    print(label)\n",
    "    print(text.numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88ff19bf-085f-4bae-b0f7-5d823a655a41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i'm 10x cooler than all of you! \""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = [text.numpy().decode('utf-8') for text, _ in train_ds]\n",
    "lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76c99383-808b-4ac6-87ec-4dc0a6e3b60c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146196"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_set = set()\n",
    "\n",
    "for l in lines:\n",
    "    for w in l.split(' '):\n",
    "        word_set.add(w)\n",
    "    \n",
    "VOCAB_SIZE = len(word_set)   \n",
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69fb2c85-a0c0-4124-a16e-41dd3795e19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0: \n",
      "  1: me~!\n",
      "  2: Bugger\n",
      "  3: time....\n",
      "  4: argument.&quot;-\n",
      "  5: ulet\n",
      "  6: @idris\n",
      "  7: @camiespice\n",
      "  8: ahead!\n",
      "  9: http://tinyurl.com/m5c8le\n"
     ]
    }
   ],
   "source": [
    "for index, w in enumerate(word_set):\n",
    "    if index >= 10:\n",
    "        break\n",
    "    print(f'{index:3}: {w}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2341d045-643f-421a-b0c3-6c79416af67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<OOV> 1\n",
      "i 2\n",
      "to 3\n",
      "the 4\n",
      "a 5\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words = VOCAB_SIZE, oov_token = '<OOV>')\n",
    "tokenizer.fit_on_texts(lines)\n",
    "word_index = tokenizer.word_index\n",
    "for index, (a, b) in enumerate(word_index.items()):\n",
    "    if index >= 5:\n",
    "        break\n",
    "    print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c5891ae-8036-466c-8d49-aa78c6e0f79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen = 0\n",
    "cnt = 0\n",
    "\n",
    "# If batch_size cannot divide total size, the remainder will be skipped\n",
    "for text_batch, label in train_ds.batch(1):\n",
    "    text_batch = map(lambda text: text.numpy().decode('utf-8'), text_batch)\n",
    "    sequences = tokenizer.texts_to_sequences(text_batch)\n",
    "    len_of_sequences = list(map(len, sequences))\n",
    "    maxlen = max(maxlen, *len_of_sequences)\n",
    "    cnt = cnt + 1\n",
    "\n",
    "print(cnt)\n",
    "maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64db17b6-3f08-41b4-b3f2-12329dc76593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   20 11880  3026   199    32    13     8     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0]\n",
      " [  321  4307   282   686     2   174   374   444   149    14   103     2\n",
      "     17  1772    13   149     3  2851     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0]]\n",
      "\n",
      "\n",
      "tf.Tensor([4 0], shape=(2,), dtype=int32)\n",
      "\n",
      "\n",
      "\n",
      "(2, 36) (2,)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def tokenize_and_pad(text_batch, label_batch):\n",
    "    text_batch = map(lambda s: s.numpy().decode('utf-8'), text_batch)\n",
    "    sequences = tokenizer.texts_to_sequences(text_batch)\n",
    "    padded = pad_sequences(sequences, \n",
    "                           maxlen = maxlen, \n",
    "                           padding ='post', \n",
    "                           truncating = 'post')\n",
    "    return padded, label_batch\n",
    "\n",
    "for text_batch, label_batch in train_ds.batch(2).take(1):\n",
    "    text_batch, label_batch = tokenize_and_pad(text_batch, label_batch)\n",
    "    print(text_batch)\n",
    "    print('\\n')\n",
    "    print(label_batch)\n",
    "    print('\\n\\n')\n",
    "    print(text_batch.shape, label_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c90290c-ba4f-46b8-8ef4-a028020af30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 36)  ->  (4,)\n",
      "tf.Tensor(\n",
      "[[   20 11880  3026   199    32    13     8     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0]\n",
      " [  321  4307   282   686     2   174   374   444   149    14   103     2\n",
      "     17  1772    13   149     3  2851     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0]\n",
      " [   56     5   305    31    25     3    49     3     6   173   434     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0]\n",
      " [24058    66     2    26    21   754     3 24059   313   378    60   233\n",
      "    995     3    58    26    11 15380  3027     8  4521     6   897     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0]], shape=(4, 36), dtype=int32)  ->  tf.Tensor([4 0 4 4], shape=(4,), dtype=int32)\n",
      "\n",
      "(4, 36)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[   20, 11880,  3026,   199,    32,    13,     8,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [  321,  4307,   282,   686,     2,   174,   374,   444,   149,\n",
       "           14,   103,     2,    17,  1772,    13,   149,     3,  2851,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [   56,     5,   305,    31,    25,     3,    49,     3,     6,\n",
       "          173,   434,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [24058,    66,     2,    26,    21,   754,     3, 24059,   313,\n",
       "          378,    60,   233,   995,     3,    58,    26,    11, 15380,\n",
       "         3027,     8,  4521,     6,   897,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_dataset_from_original_ds(ds, batch_size = 32, max_index = None):\n",
    "    text_batch_slices = []\n",
    "    label_batch_slices = []\n",
    "    for index, (text_batch, label_batch) in enumerate(ds.batch(2)):\n",
    "        if (max_index != None) and (index >= max_index):\n",
    "            break\n",
    "        text_batch, label_batch = tokenize_and_pad(text_batch, label_batch)\n",
    "        text_batch_slices.append(text_batch)\n",
    "        label_batch_slices.append(label_batch)\n",
    "    text_slices = np.concatenate(text_batch_slices)\n",
    "    label_slices = np.concatenate(label_batch_slices)\n",
    "    \n",
    "    # https://stackoverflow.com/questions/65230657/tensorflow-how-to-create-a-dataset-which-is-an-array-of-tuples\n",
    "    return tf.data.Dataset.from_tensor_slices((text_slices, label_slices)).batch(batch_size).prefetch(1)\n",
    "\n",
    "ds = get_dataset_from_original_ds(train_ds, max_index = 2)\n",
    "sample_x = None\n",
    "for x, y in ds:\n",
    "    sample_x = x\n",
    "    print(x.shape, ' -> ', y.shape)\n",
    "    print(x, ' -> ', y, end = '\\n\\n')\n",
    "    \n",
    "print(sample_x.numpy().shape)\n",
    "sample_x.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21651ef4-2045-44a5-9121-46e94ebbb4ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(None, 36), dtype=tf.int32, name=None),\n",
       " TensorSpec(shape=(None,), dtype=tf.int32, name=None))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_train_ds = get_dataset_from_original_ds(train_ds)\n",
    "final_valid_ds = get_dataset_from_original_ds(valid_ds)\n",
    "final_train_ds.element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d06175-ab07-4725-847e-443abc24303e",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd8be30d-e38c-4a90-b236-540026dbc5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def build_model_and_fit(model, train_ds, valid_ds, epochs = 500, verbose = 0):\n",
    "    early_stopping = keras.callbacks.EarlyStopping(\n",
    "        patience = 5,\n",
    "        restore_best_weights = True,\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        epochs = epochs,\n",
    "        validation_data = valid_ds,\n",
    "        callbacks = [early_stopping],\n",
    "        verbose = verbose,\n",
    "    )\n",
    "    \n",
    "    history_df = pd.DataFrame(history.history)\n",
    "    print(f'val_loss: {history_df.val_loss.min():.4f} - val_accuracy: {history_df.val_accuracy.max():.4f}')\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3420d98-a157-4674-9331-a0daf57c28e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCHS = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4572a0d4-22da-4b7a-8ef5-e912908e8e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "2500/2500 [==============================] - 28s 11ms/step - loss: 0.5630 - accuracy: 0.7198 - val_loss: 0.4964 - val_accuracy: 0.7614\n",
      "Epoch 2/500\n",
      "2500/2500 [==============================] - 25s 10ms/step - loss: 0.3513 - accuracy: 0.8539 - val_loss: 0.5597 - val_accuracy: 0.7487\n",
      "Epoch 3/500\n",
      "2500/2500 [==============================] - 25s 10ms/step - loss: 0.1941 - accuracy: 0.9253 - val_loss: 0.6777 - val_accuracy: 0.7363\n",
      "Epoch 4/500\n",
      "2500/2500 [==============================] - 26s 10ms/step - loss: 0.1079 - accuracy: 0.9613 - val_loss: 0.8167 - val_accuracy: 0.7302\n",
      "Epoch 5/500\n",
      "2500/2500 [==============================] - 26s 10ms/step - loss: 0.0628 - accuracy: 0.9780 - val_loss: 0.9855 - val_accuracy: 0.7230\n",
      "Epoch 6/500\n",
      "2500/2500 [==============================] - 25s 10ms/step - loss: 0.0386 - accuracy: 0.9868 - val_loss: 1.1640 - val_accuracy: 0.7156\n",
      "val_loss: 0.4964 - val_accuracy: 0.7614\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x19ae8e040>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(1)\n",
    "np.random.seed(1)\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Embedding(VOCAB_SIZE, 16, input_length = maxlen),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(6, activation = 'relu'),\n",
    "    keras.layers.Dense(5),\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits = True),\n",
    "    metrics = ['accuracy'],\n",
    ")\n",
    "# model.summary()\n",
    "\n",
    "build_model_and_fit(model, final_train_ds, final_valid_ds, MAX_EPOCHS, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd33d65d-ff75-4773-847b-ede3a554d0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "2500/2500 [==============================] - 24s 10ms/step - loss: 0.6621 - accuracy: 0.6754 - val_loss: 0.5221 - val_accuracy: 0.7564\n",
      "Epoch 2/500\n",
      "2500/2500 [==============================] - 24s 10ms/step - loss: 0.4663 - accuracy: 0.7929 - val_loss: 0.4903 - val_accuracy: 0.7745\n",
      "Epoch 3/500\n",
      "2500/2500 [==============================] - 24s 10ms/step - loss: 0.4123 - accuracy: 0.8235 - val_loss: 0.4873 - val_accuracy: 0.7770\n",
      "Epoch 4/500\n",
      "2500/2500 [==============================] - 24s 10ms/step - loss: 0.3698 - accuracy: 0.8461 - val_loss: 0.4962 - val_accuracy: 0.7746\n",
      "Epoch 5/500\n",
      "2500/2500 [==============================] - 24s 10ms/step - loss: 0.3307 - accuracy: 0.8663 - val_loss: 0.5131 - val_accuracy: 0.7693\n",
      "Epoch 6/500\n",
      "2500/2500 [==============================] - 24s 10ms/step - loss: 0.2939 - accuracy: 0.8840 - val_loss: 0.5399 - val_accuracy: 0.7641\n",
      "Epoch 7/500\n",
      "2500/2500 [==============================] - 24s 10ms/step - loss: 0.2601 - accuracy: 0.8997 - val_loss: 0.5764 - val_accuracy: 0.7571\n",
      "Epoch 8/500\n",
      "2500/2500 [==============================] - 23s 9ms/step - loss: 0.2299 - accuracy: 0.9126 - val_loss: 0.6163 - val_accuracy: 0.7521\n",
      "val_loss: 0.4873 - val_accuracy: 0.7770\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x199b9efa0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(1)\n",
    "np.random.seed(1)\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Embedding(VOCAB_SIZE, 16, input_length = maxlen),\n",
    "    keras.layers.GlobalAveragePooling1D(),\n",
    "    keras.layers.Dense(6, activation = 'relu'),\n",
    "    keras.layers.Dense(5),\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits = True),\n",
    "    metrics = ['accuracy'],\n",
    ")\n",
    "\n",
    "build_model_and_fit(model, final_train_ds, final_valid_ds, MAX_EPOCHS, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34991c30-2d65-4424-a71f-c2559b9eae33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "2500/2500 [==============================] - 24s 9ms/step - loss: 0.5649 - accuracy: 0.7072 - val_loss: 0.4865 - val_accuracy: 0.7737\n",
      "Epoch 2/500\n",
      "2500/2500 [==============================] - 23s 9ms/step - loss: 0.4102 - accuracy: 0.8204 - val_loss: 0.4790 - val_accuracy: 0.7747\n",
      "Epoch 3/500\n",
      "2500/2500 [==============================] - 23s 9ms/step - loss: 0.3346 - accuracy: 0.8611 - val_loss: 0.5087 - val_accuracy: 0.7632\n",
      "Epoch 4/500\n",
      "2500/2500 [==============================] - 23s 9ms/step - loss: 0.2691 - accuracy: 0.8919 - val_loss: 0.5757 - val_accuracy: 0.7476\n",
      "Epoch 5/500\n",
      "2500/2500 [==============================] - 23s 9ms/step - loss: 0.2174 - accuracy: 0.9140 - val_loss: 0.6817 - val_accuracy: 0.7352\n",
      "Epoch 6/500\n",
      "2500/2500 [==============================] - 23s 9ms/step - loss: 0.1805 - accuracy: 0.9290 - val_loss: 0.7481 - val_accuracy: 0.7300\n",
      "Epoch 7/500\n",
      "2500/2500 [==============================] - 23s 9ms/step - loss: 0.1539 - accuracy: 0.9401 - val_loss: 0.8181 - val_accuracy: 0.7230\n",
      "val_loss: 0.4790 - val_accuracy: 0.7747\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x19a564af0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(1)\n",
    "np.random.seed(1)\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Embedding(VOCAB_SIZE, 16, input_length = maxlen),\n",
    "    keras.layers.GlobalAveragePooling1D(),\n",
    "    keras.layers.Dense(32, activation = 'relu'),\n",
    "    keras.layers.Dense(32, activation = 'relu'),\n",
    "    keras.layers.Dense(32, activation = 'relu'),\n",
    "    keras.layers.Dense(5),\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits = True),\n",
    "    metrics = ['accuracy'],\n",
    ")\n",
    "\n",
    "build_model_and_fit(model, final_train_ds, final_valid_ds, MAX_EPOCHS, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f259f9d6-d151-48ef-ac99-df8d62ffbce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "2500/2500 [==============================] - 82s 33ms/step - loss: 0.5546 - accuracy: 0.7129 - val_loss: 0.4779 - val_accuracy: 0.7726\n",
      "Epoch 2/500\n",
      "2500/2500 [==============================] - 81s 32ms/step - loss: 0.3958 - accuracy: 0.8236 - val_loss: 0.4773 - val_accuracy: 0.7733\n",
      "Epoch 3/500\n",
      "2500/2500 [==============================] - 81s 32ms/step - loss: 0.3112 - accuracy: 0.8664 - val_loss: 0.5447 - val_accuracy: 0.7612\n",
      "Epoch 4/500\n",
      "2500/2500 [==============================] - 80s 32ms/step - loss: 0.2367 - accuracy: 0.9017 - val_loss: 0.6467 - val_accuracy: 0.7497\n",
      "Epoch 5/500\n",
      "2500/2500 [==============================] - 77s 31ms/step - loss: 0.1815 - accuracy: 0.9276 - val_loss: 0.7543 - val_accuracy: 0.7330\n",
      "Epoch 6/500\n",
      "2500/2500 [==============================] - 77s 31ms/step - loss: 0.1428 - accuracy: 0.9444 - val_loss: 0.8936 - val_accuracy: 0.7277\n",
      "Epoch 7/500\n",
      "2500/2500 [==============================] - 88s 35ms/step - loss: 0.1181 - accuracy: 0.9540 - val_loss: 0.9940 - val_accuracy: 0.7253\n",
      "val_loss: 0.4773 - val_accuracy: 0.7733\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x19a691400>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(1)\n",
    "np.random.seed(1)\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Embedding(VOCAB_SIZE, 32, input_length = maxlen),\n",
    "    keras.layers.GlobalAveragePooling1D(),\n",
    "    keras.layers.Dense(32, activation = 'relu'),\n",
    "    keras.layers.Dense(32, activation = 'relu'),\n",
    "    keras.layers.Dense(32, activation = 'relu'),\n",
    "    keras.layers.Dense(5),\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits = True),\n",
    "    metrics = ['accuracy'],\n",
    ")\n",
    "\n",
    "build_model_and_fit(model, final_train_ds, final_valid_ds, MAX_EPOCHS, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dff640-64c0-4601-b0d8-6bd0945d5d91",
   "metadata": {},
   "source": [
    "# Train for More Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "629981f2-54f1-4006-a3ca-8bb6a9f5ae0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "more_train_ds = tfds.load('sentiment140', split = 'train[0:10%]', as_supervised = True)\n",
    "more_valid_ds = tfds.load('sentiment140', split = 'train[10%:12%]', as_supervised = True)\n",
    "\n",
    "more_train_ds = get_dataset_from_original_ds(more_train_ds)\n",
    "more_valid_ds = get_dataset_from_original_ds(more_valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d486860c-b225-4cff-a720-19b575f887d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "5000/5000 [==============================] - 77s 15ms/step - loss: 0.5767 - accuracy: 0.7255 - val_loss: 0.4777 - val_accuracy: 0.7835\n",
      "Epoch 2/500\n",
      "5000/5000 [==============================] - 73s 15ms/step - loss: 0.4445 - accuracy: 0.8016 - val_loss: 0.4600 - val_accuracy: 0.7901\n",
      "Epoch 3/500\n",
      "5000/5000 [==============================] - 81s 16ms/step - loss: 0.4096 - accuracy: 0.8195 - val_loss: 0.4534 - val_accuracy: 0.7906\n",
      "Epoch 4/500\n",
      "5000/5000 [==============================] - 90s 18ms/step - loss: 0.3828 - accuracy: 0.8317 - val_loss: 0.4568 - val_accuracy: 0.7893\n",
      "Epoch 5/500\n",
      "5000/5000 [==============================] - 88s 18ms/step - loss: 0.3611 - accuracy: 0.8420 - val_loss: 0.4645 - val_accuracy: 0.7861\n",
      "Epoch 6/500\n",
      "5000/5000 [==============================] - 87s 17ms/step - loss: 0.3421 - accuracy: 0.8499 - val_loss: 0.4755 - val_accuracy: 0.7811\n",
      "Epoch 7/500\n",
      "5000/5000 [==============================] - 87s 17ms/step - loss: 0.3252 - accuracy: 0.8571 - val_loss: 0.4891 - val_accuracy: 0.7782\n",
      "Epoch 8/500\n",
      "5000/5000 [==============================] - 72s 14ms/step - loss: 0.3101 - accuracy: 0.8637 - val_loss: 0.5056 - val_accuracy: 0.7751\n",
      "val_loss: 0.4534 - val_accuracy: 0.7906\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x19b67fc40>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(1)\n",
    "np.random.seed(1)\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Embedding(VOCAB_SIZE, 16, input_length = maxlen),\n",
    "    keras.layers.GlobalAveragePooling1D(),\n",
    "    keras.layers.Dense(6, activation = 'relu'),\n",
    "    keras.layers.Dense(5),\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits = True),\n",
    "    metrics = ['accuracy'],\n",
    ")\n",
    "\n",
    "build_model_and_fit(model, more_train_ds, more_valid_ds, MAX_EPOCHS, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62f3b1d-8375-4454-90cf-8329b66f7e58",
   "metadata": {},
   "source": [
    "# Predict on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed3df1b1-134c-4af6-9027-e64d4e6679f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_ds = get_dataset_from_original_ds(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bc678118-3ea9-4d9b-8e6e-82a8376321ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 0, 0, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4]\n"
     ]
    }
   ],
   "source": [
    "predicted = map(tf.argmax, model.predict(final_test_ds))\n",
    "predicted = list(map(lambda x: x.numpy(), predicted))\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5f079f6c-c829-4dad-8b3f-ba86469b8cbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>real</th>\n",
       "      <th>predicted</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>@allygodinez How was the meet &amp;amp; greet?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>off to the malll ; i really dont want to do swimming in gym tomorrow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>chemistry revision is absolute balls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>heading out to CA where the waether looks sunny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Ate subway yum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>@NKAirplay That gives me visions of a Wahlberg sandwich.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>@brighit Says it's 'unavailable'-i'll have a look on Boombox</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Watchin charm school  @lalavazquez @riskybizness23  looooove you guys</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>I am also being a lazy bum this morning and currently sat with Pjs on and cuppa watching said rain while twittering  ahhhhh bliss......</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Watching HGTV on DVD before I go to work.  I don't wanna go anywhere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@AK618 ...fans starting asking for his autograph and he had to sign for them too. and he was not happy at all. I felt bad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@Jessicaveronica haha what a dumbass.... People lie about the stupidest shit!! My fam lives in Houston but I'm not there, sorry!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I got million mental and physical things to do  would 24 hours be enough?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>@Jasmine1993 i have attempted and it didnt go to well hehe i might have another go soon though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@GertieGamer Still a bit rubbish  decided to stay home today. Bit fed up.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@nicolerichie I too sleep diagonally with my boyfriend.  In my defense, he steals the covers!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>@Leah923 &amp;quot;Yeah Eric&amp;quot; is an inside joke that only 1 percent of the listeners get (and none in syndication) but it always cracks us up!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>@cswtham Growl is a kind of notification thing. So, when someone starts chatting, a notification pops up on screen. It's fine to get</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A coffee and some formula diet pulver is a good start into a week. Why diet drink? If I will get a permanent job I have to loose 9 kilos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>@rodney91 http://twitpic.com/4i3g6 - i love that game  xxxx</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    real  predicted  \\\n",
       "0   4     4           \n",
       "1   0     0           \n",
       "2   0     0           \n",
       "3   4     4           \n",
       "4   4     4           \n",
       "5   4     4           \n",
       "6   4     4           \n",
       "7   4     4           \n",
       "8   4     4           \n",
       "9   0     0           \n",
       "10  0     0           \n",
       "11  0     0           \n",
       "12  0     0           \n",
       "13  4     0           \n",
       "14  0     0           \n",
       "15  0     0           \n",
       "16  4     0           \n",
       "17  4     0           \n",
       "18  0     0           \n",
       "19  4     4           \n",
       "\n",
       "                                                                                                                                                 text  \n",
       "0   @allygodinez How was the meet &amp; greet?                                                                                                         \n",
       "1   off to the malll ; i really dont want to do swimming in gym tomorrow                                                                               \n",
       "2   chemistry revision is absolute balls                                                                                                               \n",
       "3   heading out to CA where the waether looks sunny                                                                                                    \n",
       "4   Ate subway yum                                                                                                                                     \n",
       "5   @NKAirplay That gives me visions of a Wahlberg sandwich.                                                                                           \n",
       "6   @brighit Says it's 'unavailable'-i'll have a look on Boombox                                                                                       \n",
       "7   Watchin charm school  @lalavazquez @riskybizness23  looooove you guys                                                                              \n",
       "8   I am also being a lazy bum this morning and currently sat with Pjs on and cuppa watching said rain while twittering  ahhhhh bliss......            \n",
       "9   Watching HGTV on DVD before I go to work.  I don't wanna go anywhere                                                                               \n",
       "10  @AK618 ...fans starting asking for his autograph and he had to sign for them too. and he was not happy at all. I felt bad...                       \n",
       "11  @Jessicaveronica haha what a dumbass.... People lie about the stupidest shit!! My fam lives in Houston but I'm not there, sorry!                   \n",
       "12  I got million mental and physical things to do  would 24 hours be enough?                                                                          \n",
       "13  @Jasmine1993 i have attempted and it didnt go to well hehe i might have another go soon though                                                     \n",
       "14  @GertieGamer Still a bit rubbish  decided to stay home today. Bit fed up.                                                                          \n",
       "15  @nicolerichie I too sleep diagonally with my boyfriend.  In my defense, he steals the covers!                                                      \n",
       "16  @Leah923 &quot;Yeah Eric&quot; is an inside joke that only 1 percent of the listeners get (and none in syndication) but it always cracks us up!    \n",
       "17  @cswtham Growl is a kind of notification thing. So, when someone starts chatting, a notification pops up on screen. It's fine to get               \n",
       "18  A coffee and some formula diet pulver is a good start into a week. Why diet drink? If I will get a permanent job I have to loose 9 kilos           \n",
       "19  @rodney91 http://twitpic.com/4i3g6 - i love that game  xxxx                                                                                        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "xs, ys = [], []\n",
    "\n",
    "for x, _ in test_ds:\n",
    "    xs.append(x.numpy().decode('utf-8')) \n",
    "    \n",
    "for _, y in final_test_ds:\n",
    "    ys.append(y.numpy())\n",
    "\n",
    "real = np.concatenate(ys)\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df = pd.DataFrame({'real': real, 'predicted': predicted, 'text': xs})\n",
    "# https://stackoverflow.com/questions/25351968/how-can-i-display-full-non-truncated-dataframe-information-in-html-when-conver\n",
    "with pd.option_context('display.max_colwidth', -1):     \n",
    "    display(df)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3100bc3-89a8-43b5-8bf0-6d6ce6656f8c",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "[UD187 Intro to TensorFlow for Deep Learning - Lession 9: NLP Tokenization and Embedding](https://classroom.udacity.com/courses/ud187)\n",
    "\n",
    "[Colab - Word Embeddings and Sentiment](https://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l09c04_nlp_embeddings_and_sentiment.ipynb)\n",
    "\n",
    "[Colab - Tweaking the model](https://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l09c05_nlp_tweaking_the_model.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
